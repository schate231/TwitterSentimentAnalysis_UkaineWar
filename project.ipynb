{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPSCI 723: NLP\n",
    "\n",
    "Ukraine Russia War Twitter Sentiment Analysis\n",
    "\n",
    "-Shreyas Chate\n",
    "\n",
    "-Mihir Dixit\n",
    "\n",
    "datasetlink > https://drive.google.com/file/d/1x13lLQ8mhp5jIYN8sCpt4qgGBl4whf6i/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import csv\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis.gensim_models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import  Counter\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = r\"./kaggle/input/UkraineCombinedTweetsDeduped_FEB.csv.gzip\"\n",
    "file2  = r\"./kaggle/input/UkraineCombinedTweetsDeduped_MAR.csv.gzip\"\n",
    "\n",
    "dataframe1 = pd.read_csv(file1, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
    "dataframe2 = pd.read_csv(file2, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "\n",
    "dataframef = pd.concat([dataframe1])\n",
    "dataframe = pd.concat([dataframef,dataframe2])\n",
    "\n",
    "tweet = dataframe[dataframe['language'] == 'en'] \n",
    "tweet_dataframe = tweet.loc[:,['text']]  \n",
    "tweet_dataframe.head()\n",
    "len(tweet_dataframe)\n",
    "\n",
    "c=len(tweet_dataframe)//8\n",
    "tweets_dataframe=tweet_dataframe.iloc[:c]\n",
    "len(tweets_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanTweet(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text) \n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)  \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "   \n",
    "    emoji_patterns = re.compile(pattern = \"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" \n",
    "                           u\"\\U0001F300-\\U0001F5FF\" \n",
    "                           u\"\\U0001F680-\\U0001F6FF\" \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "                           u\"\\U00002702-\\U000027B0\"  \n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    text = emoji_patterns.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "tweetsdataframe = pd.DataFrame(tweets_dataframe.text.apply(lambda x: CleanTweet(x)))\n",
    "tweetsdataframe\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "txt_length = [len(d) for d in tweetsdataframe.text]\n",
    "plt.hist(txt_length, bins=100)\n",
    "plt.title('Distribution of Tweets')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xlabel('Tweets length')\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_set = set(STOPWORDS)\n",
    "wordcloud = WordCloud(background_color='white',\n",
    "                     stopwords = stopwords_set,\n",
    "                      max_words = 300,\n",
    "                      max_font_size = 40,\n",
    "                      random_state=42).generate(str(tweetsdataframe['text']))\n",
    "\n",
    "print(wordcloud)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_en=dataframe[dataframe[\"language\"] == \"en\"].location.value_counts().sort_values(ascending=False)[:100]\n",
    "dataframe_en= pd.DataFrame(dataframe_en)\n",
    "dataframe_en=dataframe_en.reset_index()\n",
    "\n",
    "longitude,latitude=[],[]\n",
    "GeoLocation = Nominatim(user_agent=\"MyApp\")\n",
    "for i in range(len(dataframe_en)):\n",
    "    location = GeoLocation.geocode(dataframe_en['index'][i])\n",
    "    longitude.append(location.longitude)\n",
    "    latitude.append(location.latitude)\n",
    "\n",
    "dataframe_en.insert(1, \"latitude\",latitude, True)\n",
    "dataframe_en.insert(2, \"longitude\",longitude, True)\n",
    "\n",
    "LocationMap= folium.Map(tiles=\"cartodbpositron\")\n",
    "marker_cluster = MarkerCluster().add_to(LocationMap)\n",
    "\n",
    "for i in range(len(dataframe_en)):\n",
    "        latitude = dataframe_en.iloc[i]['latitude']\n",
    "        longitude = dataframe_en.iloc[i]['longitude']\n",
    "        radius=5\n",
    "        folium.CircleMarker(location = [latitude, longitude], radius=radius, fill =True).add_to(marker_cluster)\n",
    "LocationMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotBarChart(text, n=2):\n",
    "    stop=set(stopwords.words('english'))\n",
    "\n",
    "    new= text.str.split()\n",
    "    new=new.values.tolist()\n",
    "    corpus=[word for i in new for word in i]\n",
    "\n",
    "    def nGrams(corpus, n=None):\n",
    "        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) \n",
    "                      for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:10]\n",
    "\n",
    "    top_n_bigrams=nGrams(text,n)[:10]\n",
    "    x,y=map(list,zip(*top_n_bigrams))\n",
    "    sns.barplot(x=y,y=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import mpl\n",
    "mpl.rcParams['figure.figsize']=(8.0,8.0)  \n",
    "PlotBarChart(tweetsdataframe['text'],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += list(string.punctuation) \n",
    "stop_words += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "def TextTokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    return stopwords_removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pyparsing.helpers import Dict\n",
    "tweetsdataframe['text']=tweetsdataframe['text'].apply(TextTokenizer)\n",
    "TexttoDict = Dictionary(tweetsdataframe['text'])\n",
    "TexttoDict.token2id\n",
    "TweetsX = [TexttoDict.doc2bow(tweet) for tweet in tweetsdataframe['text']]\n",
    "TweetsX\n",
    "\n",
    "LdaTweets = LdaModel(TweetsX,\n",
    "                      num_topics = 10,\n",
    "                      id2word = TexttoDict,\n",
    "                      random_state = 1,\n",
    "                      passes=10)\n",
    "\n",
    "LdaTweets.show_topics()\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis = gensimvis.prepare(LdaTweets, TweetsX, dictionary=LdaTweets.id2word)\n",
    "LDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "loadClassifier = TextClassifier.load('en-sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install swifter\n",
    "\n",
    "tweets_dataframe['text'] = pd.DataFrame(tweets_dataframe.text.apply(lambda x: clean_text(x)))\n",
    "tweets_dataframe=tweets_dataframe.sample(frac=0.33,random_state=1)\n",
    "\n",
    "def SentimentPrediction(x):\n",
    "\n",
    "    newTweet = Sentence(x)\n",
    "    \n",
    "    try:        \n",
    "        loadClassifier.predict(newTweet)\n",
    "        score = newTweet.labels[0]\n",
    "        staging_score = str(score).replace(\"(\",\",\").replace(\")\",\"\")\n",
    "        \n",
    "        sentiment_score = staging_score.split(\",\")\n",
    "        \n",
    "        if \"POSITIVE\" in str(sentiment_score[0]):\n",
    "            return \"POSITIVE\", float(sentiment_score[1].strip())\n",
    "        elif \"NEGATIVE\" in str(sentiment_score[0]):\n",
    "            return \"NEGATIVE\", float(sentiment_score[1].strip())\n",
    "        else:\n",
    "            return \"NEUTRAL\", 0.00\n",
    "    except Exception:\n",
    "        print(newTweet)\n",
    "        pass  \n",
    "    \n",
    "    return \"ERROR\",0.00\n",
    "\n",
    "tweets_dataframe['Sentiment'] = \"\"\n",
    "tweets_dataframe['Sentiment_Score'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import swifter\n",
    "tweets_dataframe[\"Sentiment\"],tweets_dataframe[\"Sentiment_Score\"] =  zip(*tweets_dataframe[\"text\"].swifter.apply(SentimentPrediction))\n",
    "tweets_dataframe"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
